dataset:
  source: "arxiv"
  categories:
    - id: "cs.LG"
      weight: 4
    - id: "cs.AI"
      weight: 4
    - id: "cs.CL"
      weight: 4
    - id: "cs.CV"
      weight: 3
    - id: "stat.ML"
      weight: 3

    - id: "cs.CR"
      weight: 2
    - id: "cs.DS"
      weight: 2
    - id: "cs.IR"
      weight: 2
    - id: "cs.RO"
      weight: 2
    - id: "cs.SE"
      weight: 2
    - id: "cs.NI"
      weight: 2

    - id: "physics.bio-ph"
      weight: 3
    - id: "q-bio.BM"
      weight: 1
    - id: "q-bio.GN"
      weight: 1
    - id: "q-bio.NC"
      weight: 1
    - id: "q-bio.SC"
      weight: 1
    - id: "q-bio.CB"
      weight: 3

    - id: "physics.gen-ph"
      weight: 1
    - id: "physics.comp-ph"
      weight: 1
    - id: "physics.chem-ph"
      weight: 1
    - id: "physics.optics"
      weight: 1

    - id: "econ.GN"
      weight: 1
    - id: "q-fin.EC"
      weight: 1
    - id: "q-fin.MF"
      weight: 1
    - id: "q-fin.ST"
      weight: 1

    - id: "eess.SP"
      weight: 1
    - id: "eess.SY"
      weight: 1
    - id: "eess.IV"
      weight: 1

  save_path: "data/raw/abstracts.jsonl"
  recent_years: 5
  max_papers: 10000
  flush_every: 100
  batch_size: 256
  request_sleep_seconds: 0.5

vector_store:
  db_dir: "data/index/lancedb"
  table_name: "papers"
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  max_context_chars: 6000
  max_abstract_chars_per_doc: 1200
  initial_retrieval_k: 100
  top_k: 10
  use_reranker: true
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  reranker_max_length: 512

llm:
  # Choose backend:
  # - "ollama"    : your current local OpenAI-compatible server
  # - "llama_cpp" : fully offline GGUF inference (best for Docker/offline)
  provider: "llama_cpp"  # "ollama"

  # Ollama / OpenAI-compatible settings
  base_url: "http://localhost:11434/v1"
  api_key: "ollama"
  model: "qwen2.5:7b-instruct"

  # Shared generation settings
  temperature: 0.1
  max_tokens: 600

  # llama.cpp settings (used only when provider: "llama_cpp")
  model_path: "models/qwen2.5-7b-instruct.Q4_K_M.gguf"
  n_ctx: 4096
  n_threads: 8
  n_gpu_layers: 0
  stop:
    - "### User"
    - "### System"
